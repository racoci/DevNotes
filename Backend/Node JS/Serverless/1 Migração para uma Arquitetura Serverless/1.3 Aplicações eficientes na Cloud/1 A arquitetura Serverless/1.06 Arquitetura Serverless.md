Já entendemos como funcionam as arquiteturas tradicionais. Neste vídeo, vamos entender como podemos melhorar o serviço de correção de atividades usando uma arquitetura Serverless.

Mas, antes, precisamos entender mais alguns detalhes sobre o funcionamento das arquiteturas de microsserviços e monólitos.

Como disse anteriormente, microsserviços são sistemas distribuídos que estarão em várias máquinas e vamos comunicá-los pela rede.

Já citamos algumas vantagens e desvantagens no vídeo anterior, mas vamos discutir sobre mais alguns problemas:

## Responsabilidade (agrupamento)

Um dos problemas da arquitetura de microsserviços é em relação à responsabilidade. Quanto mais longo for o seu projeto, mais difícil garantir que um serviço não vai sobrepor a responsabilidade de outro serviço.

Por exemplo, podemos ter um serviço simples de separar as responsabilidades, como um serviço de autenticação, tudo que for relacionado a autenticação é mais simples de fazer porque é um serviço mais fechado.

Mas quando temos processos que começam no serviço e vão para outro serviço fica difícil identificasr qual é a sobreposição de responsabilidades existente. É complicado ,acaba ficando numa zona cinzenta.

## Monitoramento (visibilidade)

O monitoramento é um grande problema de microsserviços porque, como são sistemas distribuídos, você precisa garantir que o monitoramento de todos os lugares estejam bem corretos.

Porque se chamarmos um serviço e esse serviço chama outros quatro ou cinco serviços, e um deles dá algum erro e acaba não tendo o monitoramento correto, não saberemos que esse serviço está com erro. Porque não conseguiremos saber qual é a origem do serviço.

Já existem ferramentas para garantir um melhor monitoramento. Mas o problema dessas ferramentas é que elas são caras e tornam-se um custo a mais no seu desenvolvimento.

## Compartilhamento (segurança)

O terceiro problema é o compartilhamento de segredos, coisas pela rede, etc. Como sempre, o compartilhamento é bastante complicado.

## Servidores

O maior problema ao lidar com serviços que estão em máquinas virtuais é lidar com o servidor. Existem vários problemas, os três maiores são os seguintes:

1 - Configuração (manutenção e backup)

Precisamos configurar toda a máquina e fazer backups para garantir que não perderemos dados.

2 - Monitoramento (disponibilidade)

Precisamos garantir que o servidor fique online, mas para isso temos que ter um monitoramento constante. É bem trabalhoso fazer esse tipo de coisa.

3 - Custo (sempre ativo)

Por fim, temos o custo. Além do custo de manter o servidor, também existe o custo de segurança, por exemplo, quando fazemos manutenção e atualização do sistema operacional.

O custo é o maior dos problemas, com certeza. Porque o servidor está sempre online, não pagamos o servidor só pelos momentos em que ele está computando alguma coisa. Estamos pagando uma máquina virtual e essa máquina vai ficar online 100% do tempo e vai ter momentos de computação ociosa.

Esse é o maior agravante para nosso sistema de provas, esse sistema só precisa estar online quando as pessoas forem fazer as provas. Então não tem porquê ficar online o tempo inteiro.

## Nanosserviços

Então, podemos trabalhar com uma evolução dos microsserviços: os nanoserviços. Essa é a ideia de separar nossos serviços no menor nível possível. E qual é o menor nível possível de uma API? É uma rota, por exemplo.

Então, pegaríamos uma rota, uma função, e colocaríamos essa função para rodar em algum lugar. Mas essa função, diferente dos servidores, não estará ligada o tempo inteiro, vai responder a determinados eventos, por exemplo, a uma requisição HTTP.

Foi dessa ideia que surgiu o movimento Serverless, que é representado pela letra grega **Lambda**. Principalmente por causa do primeiro provedor Serverless, o **AWS Lambda**.

## Serverless

O Serverless resolve grande parte dos problemas que temos com a arquitetura tradicional. Alguns deles são:

1 - Configuração (delegada para o servidor)

O primeiro problema que não teremos é a configuração, pois delegamos essa configuração para provedor do serviço.

2 - Monitoramento (nativo)

Como o provedor precisa cobrar de acordo com requisições, com execuções e tempo de resposta, toda função Serverless vai ter monitoramento ativo.

3 - Escala (virtualmente infinita)

Vamos rodar a função em um computador compartilhado com milhares de funções de outras pessoas. Então, basicamente, usaremos toda a estrutura do provedor para rodar a nossa função. Poderemos escalá-la de acordo com o provedor e de acordo com o que precisarmos.

4 - Custo (baseado em execuções)

O custo diminui porque será baseado em execuções.

Geralmente, três fatores determinam o preço de uma função:

- quantidade de execuções
- tempo que demorou para a função serem executadas
- quantidade de memória alocada para a função ser executada

Fica mais barato rodar uma função porque será compartilhado com toda a infraestrutura Serverless. Fica em torno de 60% mais barato.

5 - Disponibilidade (sempre disponível)

Os provedores vão garantir, quase sempre, que teremos a disponibilidade quando precisarmos dessas funções.

6 - Performance (alta devido à leveza)

Quanto mais rápido, menor será o custo. Elas são altamente performáticas, principalmente devido à leveza de como elas executam.

Mas ao ouvir ou ler a palavra "Serverless", você pode achar que não terá mais servidores. Não significa isso, não será a forma tradicional de ter um servidor.

Ao trabalhar com Serverless, vamos delegar o gerenciamento dos servidores para os provedores de cloud. Geralmente, nossas funções serão executadas numa arquitetura de contêineres. Mas não precisamos gerenciar isso.

## Desvantagens

Mas o Serverless também tem algumas desvantagens, principalmente em relação a limitações. O tempo de execução é limitado, podemos executar uma função por determinado tempo e depois ela vai ser automaticamente cortada.

Uma das limitações que teremos é em relação ao estado. Funções Serverless não mantém estado, tudo o que você precisa guardar não será compartilhado entre as próximas execuções. Será guardado tudo em memória.

Outro problema é o **_cold start_**. O que é o _cold start_? Imagine o seguinte: você tem um carro que ficou completamente parado por meses, ele vai demorar mais tempo para ligar. O mesmo acontece com funções. Quanto mais executarmos uma função, ela será cacehada e será executada mais rápido nas próximas vezes.

Mas, se não executarmos algumas funções durante algum tempo, elas caem no que chamamos de _cold start_, elas terão que inicializar o ambiente todo novamente, o que adiciona alguns milisegundos na sua execução inicial. O que pode ou não ser um problema, depende da sua aplicação.

O principal problema do Serverless é que estamos delegando tudo para o provedor. Ao fazer isso temos uma desvantagem chamada "**_vendor lock-in_**". Porque a maioria dos provedores não quer que você saia deles para fazer alguma coisa, então acabamos utilizando cada vez mais serviços do provedor e acabamos preso ao provedor.

- Limitações (tempo, memória, uso de CPU)
- Estado (funções não mantém estado)
- _Cold start_ (cache de execuções)
- _Vendor lock-in_

Mas existem formas de resolver problemas como o _vendor lock-in_, veremos sobre isso na próxima aula.