**Antônio**: Já utilizamos um arquivo CSV e fazemos seu upload do servidor S3 local. Falta conseguir processar esses arquivos para fazer algumas requisições na API e poder cadastrar estudantes.

Isso porque os dados do CSV vêm apenas como _string_, mas seria melhor irem em uma estrutura de dados que pudéssemos manipular para fazer uma requisição `POST` futuramente. Por exemplo, seria mais interessante se cada estudante fosse representado por um **objeto**.

## Função para converter dados CSV

Para isso, vamos em "cadastro_batch > local > `index.js`". Na função `cadastrarAlunos` já armazenamos os dados do CSV do _bucket_ na constante `dadosArquivo` a partir da função `obtemDadosDoCsvDoBucket`.

A partir de `dadosArquivo`, queremos ter uma lista de estudantes para poder utilizar e depois fazer o cadastro em lote, por exemplo.

Para isso, vamos apagar o `console.log()` de `dadosArquivo`. Em seu lugar, vamos escrever um `const alunos` que recebe uma função `converteDadosCsv()`, passando `dadosArquivo` como parâmetro.

Ainda vamos criar essa função `converteDadosCsv()`, pois é nela que queremos fazer o processamento do arquivo.

> `index.js`:

```js
// código omitido…

module.exports.cadastrarAlunos = async (evento) => {
  const eventoS3 = evento.Records[0].s3;

  const nomeBucket = eventoS3.bucket.name;
  const chaveBucket = decodeURIComponent(eventoS3.object.key.replace(/\+/g, " "));

  const dadosArquivo = await obtemDadosDoCsvDoBucket(nomeBucket, chaveBucket);

  const alunos = await converteDadosCsv(dadosArquivo);
};
```

No explorador, vamos criar um novo arquivo chamado `converteDadosCsv.js` dentro da pasta "cadastro_batch". Criamos fora da pasta "local", porque sabemos que podemos utilizar essa função sem nenhuma alteração até em momentos de _deploy_. Pois, vai ser uma função JavaScript que processa uma _string_ e devolve uma lista de estudantes.

Nesse arquivo, criamos uma função chamada `converteDadosCsv()` que recebe `dados` como parâmetro. Fora da fução, vamos digitar `module.exports` igual a um objeto que contém `converteDadosCsv` para exportar essa função.

> `converteDadosCsv.js`:

```js
function converteDadosCsv(dados) {

}

module.exports = { converteDadosCsv };
```

Após salvar o arquivo, podemos voltar para `index.js` para importar essa função `converteDadosCsv` utilizada dentro de `cadastrarAlunos`. Fazemos o _auto import_ com "Ctrl + Espaço".

> `index.js`:

```js
const { converteDadosCsv } = require("../converteDadosCsv");
const { fazUploadNoBucket, obtemDadosDoCsvDoBucket } = require("./servidorS3");

// código omitido…
```

## Instalação da biblioteca Fast-CSV

Existem bibliotecas do Node que nos ajudam a processar dados CSV, pois o que queremos fazer é uma operação bastante comum. Dentre as várias alternativas, vamos utilizar a **biblioteca Fast-CSV** que é bastante popular.

Para utilizá-la, vamos abrir o terminal integrado e interromper o servidor com "Ctrl + C" e digitar o comando `npm i` para instalar a versão `fast-csv@4.3.6`.

```css
npm i fast-csv@4.3.6
```

Para confirmar se a instalação do `fast-csv` foi bem sucedida, podemos conferir as dependências no arquivo `package.json`:

> `package.json`:

```json
  "dependencies": {
    "@aws-sdk/client-s3": "^3.295.0",
    "fast-csv": "^4.3.6"
  }
```

## Utilização biblioteca Fast-CSV

Agora, vamos voltar para `converteDadosCsv.js`. Primeiro, vamos importar um método da biblioteca Fast_CSV que vamos utilizar. Para isso, digitamos `const` e colocamos `parse` entre chaves que vai receber `require()` da biblioteca `fast-csv`.

Dentro da função `converteDadosCsv()`, vamos criar uma `const` chamada `stream` igual à `parse()`. Essa _stream_ se baseia nas _streams_ do Node.js apropriadas para lidar com arquivos de tamanho muito grande de uma forma mais performática.

Em uma nova linha, vamos escrever `stream.write()` passando `dados` como parâmetro. O termo "_write_" significa "escrever" em inglês. Na próxima linha, também vamos escrever `stream.end()` sem parâmetro. O "_end_" significa "final".

> `converteDadosCsv.js`:

```js
const { parse } = require("fast-csv");

function converteDadosCsv(dados) {
  const stream = parse()

  stream.write(dados);
  stream.end();
}
```

Como funciona a `stream`? Assim que inserimos `dados` dentro da `stream` com o método `write()`, podemos executar determinadas funções de acordo com a inserção de dados na `stream`.

Onde registramos essas funções? Após a execução do método `parse()`. Para melhor legibilidade, damos um "Enter" antes do ponto e vírgula e escrevemos `.on()`.

No primeiro parâmetro do `on()`, vamos escutar um evento já previsto pela biblioteca chamado `data` entre aspas duplas. O termo "_data_" significa "dados" em inglês.

Como segundo parâmetro, passamos uma função _callback_ que recebe esses dados como parâmetro. Nesse caso, o dado é `aluno`. Depois, vamos dar um `console.log()` em `aluno` para verificar o que acontece.

Também podemos registrar funções _callback_ para outros eventos previstos por essa biblioteca. Além do `data`, podemos escrever outro `.on()` para ouvir o evento de `error` entre aspas duplas como primeiro parâmetro. Assim, caso haja algum erro na leitura dessa arquivo CSV, podemos executar um determinado código.

Como segundo parâmetro passamos uma função _callback_ recebendo `erro` e dando `console.log()` no `erro`.

Para finalizar, vamos escrever mais um `.on()` que vai escutar o evento de `end` entre aspas duplas. A função _callback_ vai ser executada assim que o processamento do arquivo for finalizado. Por isso, como o segundo parâmetro colocamos uma função _callback_ sem parâmetro e que dá um `console.log()` com a frase `O arquivo CSV foi processado.` entre aspas duplas.

```js
const { parse } = require("fast-csv");

function converteDadosCsv(dados) {
  const stream = parse()
    .on("data", (aluno) => console.log(aluno))
    .on("error", (erro) => console.log(erro))
    .on("end", () => console.log("O arquivo CSV foi processado."));

  stream.write(dados);
  stream.end();
}
```

Após salvar o arquivo, vamos abrir o terminal integrado para saber se os `console.log()` vão funcionar quando recebermos os dados do CSV como parâmetro. Executamos o comando para reiniciar o servidor:

```undefined
sls offline
```

> Server ready: [http://localhost:3000](http://localhost:3000/)

Agora que o servidor está pronto, vamos para o Thunder Client ("Ctrl + Shift + R") e vamos enviar novamente a requisição "POST/alunos/batch".

> POST [http://localhost:3000/alunos/batch](http://localhost:3000/alunos/batch)

```json
{
    "mensagem": "Simulando upload de arquivo…"
}
```

Aparentemente não houve nenhum erro. Ao abrir o terminal integrado, tivemos como resposta quatro listas com as _strings_ das informações dos alunos que estavam no arquivo CSV.

```css
[ 'Nome', 'Email' ]
[ 'Antônio Evaldo', 'antonio.evaldo@exemplo.com' ]
[ 'Samuel Teixeira', 'samuel.teixeira@exemplo.com' ]
[ 'Carla Rejane', 'carla.rejane@exemplo.com' ]
O arquivo CSV foi processado.
```

Esse formato já é interessante, mas ainda não é o ideal para o nosso caso. Por exemplo, queríamos ter objetos com uma propriedade chamada `Nome` com o nome do aluno e outra propriedade `Email` com o e-mail do aluno. Isso facilitaria bastante o nosso código. A biblioteca Fast-CSV também fornece soluções para o que queremos.

## Converter dados CSV para objeto

Vamos voltar para o arquivo `converterDadosCsv.js`. No método `parse()`, vamos passar como parâmetro um objeto que vai ter a propriedade `headers` com valor `true`.

> `converteDadosCsv.js`:

```js
function converteDadosCsv(dados) {
  const stream = parse({ headers: true })

// código omitido…

}
```

Vamos ver o que acontece? No terminal integrado, novamente vamos reiniciar o servidor com "Ctrl + C" e o comando `sls offline`. Após o servidor estar de pé, vamos enviar novamente a requisição "POST/alunos/batch" no Thunder Client.

Como resposta no terminal integrado temos três objetos. Por exemplo, temos um objeto com a propriedade `Nome` igual à `Antônio Evaldo` e a propriedade `Email` com o e-mail de exemplo.

```css
{ Nome: 'Antônio Evaldo', Email: 'antonio.evaldo@exemplo.com' }
{ Nome: 'Samuel Teixeira', Email: 'samuel.teixeira@exemplo.com' }
{ Nome: 'Carla Rejane', Email: 'carla.rejane@exemplo.com' }
O arquivo CSV foi processado.
```

Basicamente, quando passamos a propriedade `headers` com valor `true`, o `parse()` identifica que a primeira linha do CSV são os _headers_, ou seja, os cabeçalhos. Assim, nas demais linhas, o método entende que o primeiro valor é um `Nome` e o segundo é um `Email`.

Temos um último detalhe a ser modificado. No terminal integrado, perceba que as propriedades dos objetos obtidos como resposta estão com a letra inicial maiúscula. Por exemplo, `Nome` com N maiúsculo.

O ideal seria uma propriedade com todas as letras minúsculas no Java Script. Afinal, quando fizermos uma requisição para a API precisamos de tudo minúsculo.

Para isso, voltamos ao método `parse()` da função `converteDadosCsv()`. Ao invés de passar `true` no `headers`, vamos trocar para uma lista com os cabeçalhos que queremos: `nome` e `email` ambos entre aspas duplas e em minúsculas.

Também é preciso adicionar no `parse()` mais uma propriedade chamada `renameHeaders` com valor `true`. Assim, o Fast-CSV entende que queremos renomear esses cabeçalhos.

> `converteDadosCsv.js`:

```js
function converteDadosCsv(dados) {
  const stream = parse({ headers: ["nome", "email"], renameHeaders: true })

// código omitido…

}
```

Após salvar o arquivo, vamos reiniciar o servidor no terminal integrado com "Ctrl + C" e o comando `sls offline`. Feito isso, vamos enviar novamente a requisição "POST/alunos/batch" no Thunder Client.

Com isso, os objetos vieram com as propriedades `nome` e `email` em minúsculas na resposta do terminal integrado - assim como queríamos.

```css
{ nome: 'Antônio Evaldo', email: 'antonio.evaldo@exemplo.com' }
{ nome: 'Samuel Teixeira', email: 'samuel.teixeira@exemplo.com' }
{ nome: 'Carla Rejane', email: 'carla.rejane@exemplo.com' }
O arquivo CSV foi processado.
```

Com isso, já podemos tratar essas informações para realmente realizar as requisições na API a partir do próximo vídeo.