# 3.03 Utilizando um arquivo CSV

**Antônio:** Já conseguimos simular algumas etapas localmente do fluxo de cadastro de pessoas estudantes. Desse modo, já temos um servidor S3 local funcionando e conseguimos interagir com ele de algumas formas, como fazer upload de um arquivo e obter suas informações em seguida.

Vamos relembrar o fluxo de cadastro:

- Formulário Front-End;
- Upload do CSV no Bucket S3;
- Processar CSV;
- Cadastro em Batch na API.

Como explicado, estamos executando a segunda e terceira etapa de forma local. Agora, estamos na etapa de **processar CSV**. Esse seria o primeiro passo da execução da função lambda. Já começamos essa etapa ao resgatar o conteúdo do arquivo que fizemos upload no _bucket_ local, mas agora precisamos usar um CSV de verdade. Afinal, só cadastramos um texto `12345`.

## Organização do projeto

Antes disso, vamos fazer uma organização melhor no projeto. Praticamente tudo está dentro do arquivo `index.js`.

Primeiro, vamos criar uma pasta na raiz do projeto chamada "cadastro_batch". Dentro dela, vamos criar mais uma pasta chamada "local" para deixar arquivos que vão ser apenas para desenvolvimento local. Desse modo, pensamos no futuro quando utilizaremos códigos que vão apenas para produção.

O primeiro arquivo que vamos mover para dentro da pasta "local" será o `index.js`, ou seja, arrastamos o arquivo e confirmamos que desejamos movê-lo.

Dentro da pasta "local", vamos criar um arquivo chamado `servidorS3.js`. Nele, vamos deixar as funções que interagem com o servidor S3 - o qual por enquanto é local.

Acessamos o arquivo `index.js` para recortar com "Ctrl + X" as funções `criaClientesS3Local()`, `fazUploadNoBucket()` e `obtemDadosDoCsvDoBucket()`. Também vamos recortar o `require()` do começo do arquivo.

> Recorte e cole:

```js
const { S3Client, PutObjectCommand, GetObjectCommand } = require("@aws-sdk/client-s3");

function criaClienteS3Local() {
  return new S3Client({
    forcePathStyle: true,
    credentials: {
      accessKeyId: "S3RVER",
      secretAccessKey: "S3RVER"
    },
    endpoint: "http://localhost:4569"
  });
}

async function fazUploadNoBucket() {
  const cliente = criaClienteS3Local();

  const comandoUpload = new PutObjectCommand({
    Bucket: "alunos-csv-local",
    Key: "teste.csv",
    Body: Buffer.from("12345")
  });

  await cliente.send(comandoUpload);
}

async function obtemDadosDoCsvDoBucket(nome, chave) {
  const cliente = criaClienteS3Local();

  const comando = new GetObjectCommand({
    Bucket: nome,
    Key: chave
  });

  const resposta = await cliente.send(comando);
  const dadosCsv = await resposta.Body.transformToString("utf-8");

  return dadosCsv;
}
```

Vamos colar o `require()` e essas três funções no `servidorS3.js` com o atalho "Ctrl + V".

Ao final do arquivo `servidorS3.js`, vamos escrever um `module.exports` igual à um objeto que vai exportar as funções `fazUploadNoBucket` e `obtemDadosDoCsvDoBucket`. A função `criaClienteS3Local()` não precisa ser exportada porque ela é utilizada apenas pelas outras duas funções desse arquivo.

Foi preciso mover também o `require()`, pois `S3Client`, `PutObjectCommand` e `GetObjectCommand` são os comandos que vão ser utilizados pelas funções que interagem com o _bucket_ local.

Perceba como o ficou melhor separado dessa forma:

> `servidorS3.js`:

```js
const { S3Client, PutObjectCommand, GetObjectCommand } = require("@aws-sdk/client-s3");

function criaClienteS3Local() {
  return new S3Client({
    forcePathStyle: true,
    credentials: {
      accessKeyId: "S3RVER",
      secretAccessKey: "S3RVER"
    },
    endpoint: "http://localhost:4569"
  });
}

async function fazUploadNoBucket() {
  const cliente = criaClienteS3Local();

  const comandoUpload = new PutObjectCommand({
    Bucket: "alunos-csv-local",
    Key: "teste.csv",
    Body: Buffer.from("12345")
  });

  await cliente.send(comandoUpload);
}

async function obtemDadosDoCsvDoBucket(nome, chave) {
  const cliente = criaClienteS3Local();

  const comando = new GetObjectCommand({
    Bucket: nome,
    Key: chave
  });

  const resposta = await cliente.send(comando);
  const dadosCsv = await resposta.Body.transformToString("utf-8");

  return dadosCsv;
}

module.exports = { 
  fazUploadNoBucket,
  obtemDadosDoCsvDoBucket
}
```

Após salvar o arquivo, podemos voltar em `index.js` para importar as funções recortadas para o código continuar a funcionar. Para importar o `fazUploadNoBucket()` utilizado no `try-catch`, vamos colocar o cursor do mouse ao final do nome da função e dar "Ctrl + Espaço" para aceitar a sugestão de _auto import_ do VSCode.

Também vamos importar a função `obtemDadosDoCsvDoBucket()` utilizado no `const dadosArquivo` com o mesmo processo de _auto import_.

> `index.js`:

```js
const { fazUploadNoBucket, obtemDadosDoCsvDoBucket } = require("./servidorS3");

module.exports.simulandoUploadDoCsv = async (evento) => {
  try {
    await fazUploadNoBucket();

    return {
      statusCode: 200,
      body: JSON.stringify({
        mensagem: "Simulando upload de arquivo..."
      })
    };
  } catch (erro) {
    return {
      statusCode: erro.statusCode || 500,
      body: JSON.stringify(erro)
    };
  }
}

module.exports.cadastrarAlunos = async (evento) => {
  const eventoS3 = evento.Records[0].s3;

  const nomeBucket = eventoS3.bucket.name;
  const chaveBucket = decodeURIComponent(eventoS3.object.key.replace(/\+/g, " "));

  const dadosArquivo = await obtemDadosDoCsvDoBucket(nomeBucket, chaveBucket);

  console.log(dadosArquivo);
};
```

Como mudamos os arquivos de local, também precisamos atualizar o caminho do `handler` de cada função no arquivo `serverless.yml`.

No `handler` da função `simulandoUploadDoCsv`, vamos acrescentar `cadastro_batch/local/` antes de `index.simulandoUploadDoCsv` para atualizar o caminho.

Também vamos acrescentar `cadastro_batch/local/` no início do `handler` da função `cadastrarAlunos`. Ou seja, o caminho será `cadastro_batch/local/index.cadastrarAlunos`.

> `serverless.yml`:

```yml
# código omitido…

functions:
  simulandoUploadDoCsv:
    handler: cadastro_batch/local/index.simulandoUploadDoCsv
    events:
      - httpApi:
          path: /alunos/batch
          method: post

  cadastrarAlunos:
    handler: cadastro_batch/local/index.cadastrarAlunos
    events:
      - s3:
          bucket: alunos-csv-local
          event: s3:ObjectCreated:*
          rules:
            - suffix: .csv

# código omitido…
```

Após salvar o arquivo, vamos abrir o terminal integrado do VSCode com o atalho "Ctrl + `" para testar se o código continua funcionando. Caso necessário, pare o servidor com "Ctrl + C".

Vamos digitar o seguinte comando para reiniciar o servidor:

```undefined
sls offline
```

> Server ready: [http://localhost:3000](http://localhost:3000/)

Também vamos fazer uma requisição `POST` no Thunder Client para verificar o funcionamento. Vamos clicar no ícone de reticências no menu vertical e escolher "Thunder Client" (ou atalho "Ctrl + Shift + R").

Em "Activity", vamos clicar em "POST/alunos/batch" para abrir a requisição. Em seguida, vamos clicar no botão "_Send_" no canto superior direito para enviá-la.

> POST [http://localhost:3000/alunos/batch](http://localhost:3000/alunos/batch)

```json
{
    "mensagem": "Simulando upload de arquivo…"
}
```

Abrimos o terminal integrado novamente, onde aparece o texto `12345` sinalizando que o código ainda funciona.

> 12345

## Utilizar um arquivo CSV

Agora, podemos utilizar um arquivo CSV de verdade. Na atividade "Preparando o ambiente" dessa aula, deixamos um [arquivo CSV](https://github.com/alura-cursos/serverless-framework-2-lambda/blob/aula-3/cadastrar_alunos.csv) pronto para você baixar.

Abrimos o explorador de arquivos no computador para arrastar o arquivo chamado `cadastrar_alunos.csv` até a pasta "cadastro_batch > local" no explorador do VSCode.

Com isso, é aberto um arquivo com o `Nome` e `Email` de três estudantes. A sigla CSV indica um arquivo _Comma-separated values_, ou seja, **valores separados por vírgulas**. Por isso, essas informações estão separadas por vírgula.

> `cadastrar_alunos.csv`:

```js
Nome,Email
Antônio Evaldo,antonio.evaldo@exemplo.com
Samuel Teixeira,samuel.teixeira@exemplo.com
Carla Rejane,carla.rejane@exemplo.com
```

Vamos usar esse arquivo como exemplo para fazer um upload. Por isso, vamos até o arquivo `servidorS3.js` onde fazemos a simulação do upload com a função `fazUploadNoBucket()`.

Estamos usando o `PutObjectCommand()`, onde escrevemos o `teste.csv` na `Key` e utilizamos o conteúdo de `12345` como exemplo.

Antes de criar o `const comandoUpload`, vamos escrever `const nomeArquivo` que vai receber `cadastrar_alunos.csv`. Assim, vamos utilizar o próprio Node para ler o arquivo na pasta "local".

Na próxima linha, vamos escrever `const caminhoArquivo` que recebe o método `join()` que vai juntar caminhos de arquivos.

Esse `join()` precisa ser importado de um modo chamado `path` do Node. No começo do arquivo, vamos escrever `const` e `join` entre chaves que vai receber `require("path")`.

O primeiro parâmetro do `join()` vai ser `__dirname` que se refere ao diretório atual. A partir dele, vamos acessar o arquivo `cadastar_alunos.csv` que guardamos na constante `nomeArquivo`. Por isso, `nomeArquivo` é o segundo parâmetro.

Em seguida, vamos escrever `const dadosCsv` que recebe um `await` e `readFile`.

Temos duas sugestões de importação do `readFile`: do caminho `fs` e do `fs/promises`. Vamos utilizar a segunda opção para poder usar a palavra-chave `await`. Com isso, o `readFile` é importado automaticamente no início do arquivo.

O primeiro parâmetro de `readFile()` vai ser a constante `caminhoArquivo`. Já o segundo vai ser a codificação desse arquivo. Nesse caso, `utf-8` entre aspas duplas.

> `servidorS3.js`:

```js
const { S3Client, PutObjectCommand, GetObjectCommand } = require("@aws-sdk/client-s3");
const { readFile } = require("fs/promises");
const { join } = require("path");

// código omitido…

async function fazUploadNoBucket() {
  const cliente = criaClienteS3Local();

  const nomeArquivo = "cadastrar_alunos.csv";
  const caminhoArquivo = join(__dirname, nomeArquivo);
  const dadosCsv = await readFile(caminhoArquivo, "utf-8");

  const comandoUpload = new PutObjectCommand({
    Bucket: "alunos-csv-local",
    Key: "teste.csv",
    Body: Buffer.from("12345")
  });

  await cliente.send(comandoUpload);
}

// código omitido…
```

Com isso, podemos passar a constante `dadosCsv` no `Body` de `PutObjectCommand()`. Ou seja, substituímos `Buffer.from("12345")` por `dadosCsv`.

Também vamos mudar a `Key` de `PutObjectCommand()`. Ao invés de `teste.csv`, vamos colocar a constante `nomeArquivo`.

```js
// código omitido…

async function fazUploadNoBucket() {
  const cliente = criaClienteS3Local();

  const nomeArquivo = "cadastrar_alunos.csv";
  const caminhoArquivo = join(__dirname, nomeArquivo);
  const dadosCsv = await readFile(caminhoArquivo, "utf-8");

  const comandoUpload = new PutObjectCommand({
    Bucket: "alunos-csv-local",
    Key: nomeArquivo,
    Body: dadosCsv
  });

  await cliente.send(comandoUpload);
}

// código omitido…
```

Ao salvar o arquivo, podemos reiniciar o servidor local para verificar se conseguimos fazer upload do arquivo CSV e posteriormente obtê-lo com a função lambda.

No terminal integrado, reiniciamos o servidor com "Ctrl + C" e o comando `sls offline`.

> Server ready: [http://localhost:3000](http://localhost:3000/)

No Thunder Client, vamos novamente enviar a requisição "POST/alunos/batch".

> POST [http://localhost:3000/alunos/batch](http://localhost:3000/alunos/batch)

```json
{
    "mensagem": "Simulando upload de arquivo…"
}
```

Após fazer o _send_, podemos abrir o terminal integrado mais uma vez para verificar se aparecem as informações do arquivo CSV.

> Nome,Email
> 
> Antônio Evaldo,antonio.evaldo@exemplo.com
> 
> Samuel Teixeira,samuel.teixeira@exemplo.com
> 
> Carla Rejane,carla.rejane@exemplo.com

Sabemos que foi feito o upload no _bucket_ local, pois apareceram os nomes e e-mails dos estudantes.

Por fim, podemos expandir a pasta "buckets" no explorador do VSCode para visualizar que agora tem mais alguns arquivos nela que se referem ao CSV.

Inclusive, podemos apagar da pasta "buckets" os três arquivos de `teste.csv` que estávamos utilizando:

- `teste.csv._S3rver_metadata.json`
- `teste.csv._S3rver_object`
- `teste.csv._S3rver_object.md5`

Com isso, utilizamos o arquivo CSV. A partir do próximo vídeo, vamos começar a processar os dados desse arquivo. Te espero lá.