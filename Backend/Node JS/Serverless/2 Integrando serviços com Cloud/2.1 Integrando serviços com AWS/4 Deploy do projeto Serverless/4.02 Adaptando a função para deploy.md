# 4.02 Adaptando a função para deploy

Nós já finalizamos o fluxo de interação entre um servidor S3 com a função Lambda. Simulamos um servidor S3 localmente, fizemos o upload de um arquivo e a nossa função Lambda reagiu como esperado a esse evento do S3. Agora, começaremos a adaptar para poder fazer o _deploy_ na conta da AWS.

## Separando a função

Dentro da pasta "cadastro_batch > local", temos o arquivo `index.js`. Nele, temos a função `cadastrarAlunos()`, que é do que realmente faremos _deploy_. É interessante manter o arquivo `index.js` como está, caso queiramos testar localmente no futuro, então vamos copiar a função `cadastrarAlunos()` e colocá-la em outro arquivo.

Dentro da pasta "cadastro_batch", criaremos uma pasta chamada "aws". É nela em que colocaremos os arquivos que vão para a produção. Portanto, adicionaremos um arquivo chamado `index.js` e colaremos a função `cadastrarAlunos()`:

```js
// cadastro_batch > aws > index.js

module.exports.cadastrarAlunos = async (evento) => {
  try {
    const eventoS3 = evento.Records[0].s3;

    const nomeBucket = eventoS3.bucket.name;
    const chaveBucket = decodeURIComponent(eventoS3.object.key.replace(/\+/g, " "));

    const dadosArquivo = await obtemDadosDoCsvDoBucket(nomeBucket, chaveBucket);

    const alunos = await converteDadosCsv(dadosArquivo);

    await cadastrarAlunosNoBd(alunos);

    console.log("Cadastro dos alunos realizado com sucesso!");
  } catch (erro) {
    console.log(erro);
  }
};
```

Vamos revisar rapidamente esse código. Estamos utilizando algumas funções como `obtemDadosDoCsvDoBucket()`, `converteDadosCsv()` e `cadastrarAlunosNoBd()`. Vamos importar as duas últimas, no início desse arquivo:

```js
const { cadastrarAlunosNoBd } = require("../cadastrarAlunosNoBd");
const { converteDadosCsv } = require("../converteDadosCsv");

// ...
```

Anteriormente, deixamos essas funções preparadas fora da pasta "local", porque sabíamos que iríamos reutilizá-las. Então, a implementação delas permanece a mesma.

Já a função `obtemDadosDoCsvDoBucket()` interage com o S3, então é preciso fazer algumas adaptações. Vamos abrir "cadastro_batch > local > `servidorS3.js`", onde a função `obtemDadosDoCsvDoBucket()` está definida. Vamos copiá-la.

Voltando ao "cadastro_batch > aws > `index.js`", colaremos a função logo após as importações:

```js
const { cadastrarAlunosNoBd } = require("../cadastrarAlunosNoBd");
const { converteDadosCsv } = require("../converteDadosCsv");

async function obtemDadosDoCsvDoBucket(nome, chave) {
  const cliente = criaClienteS3Local();

  const comando = new GetObjectCommand({
    Bucket: nome,
    Key: chave
  });

  const resposta = await cliente.send(comando);
  const dadosCsv = await resposta.Body.transformToString("utf-8");

  return dadosCsv;
}

// ...
```

A seguir, faremos algumas adaptações. Nesse código, temos a função `criaClienteS3Local()`, porém não criaremos mais um cliente S3 local, mas um cliente S3 de verdade. Portanto, substituiremos essa chamada por `new S3Client()`, passando um objeto vazio. Também faremos a importação correspondente no início do arquivo:

```js
const { S3Client } = require("@aws-sdk/client-s3");
const { cadastrarAlunosNoBd } = require("../cadastrarAlunosNoBd");
const { converteDadosCsv } = require("../converteDadosCsv");

async function obtemDadosDoCsvDoBucket(nome, chave) {
  const cliente = new S3Client({});

  const comando = new GetObjectCommand({
    Bucket: nome,
    Key: chave
  });

  const resposta = await cliente.send(comando);
  const dadosCsv = await resposta.Body.transformToString("utf-8");

  return dadosCsv;
}

// ...
```

Quando conectamos com o S3, o código fica mais simples. Não precisamos passar propriedades para conectar com o servidor local, basta informar um objeto vazio e funcionará. A seguir, precisamos cadastrar a função `cadastrarAlunos()` no arquivo `serverless.yml`.

## Cadastrando a função

Para diferenciar a função de cadastro local e a função que vai para produção, vamos deixar seus nomes mais claros. Em `serverless.yml`, dentro de `functions`, substituiremos `cadastrarAlunos` por `cadastrarAlunosDoBucketLocal` (nas linhas 16e 17):

```yml
functions:
# ...

  cadastrarAlunosDoBucketLocal:
    handler: cadastro_batch/local/index.cadastrarAlunosDoBucketLocal
    events:
      - s3:
          bucket: alunos-csv-local
          event: s3:ObjectCreated:*
          rules:
            - suffix: .csv

# ...
```

Vamos alterar o nome da função no arquivo "cadastro_batch > local > `index.js`" também:

```js
// ...

module.exports.cadastrarAlunosDoBucketLocal = async (event) => {

  // ...

};
```

Voltando ao arquivo `serverless.yml`, reutilizaremos a estrutura de `cadastrarAlunosDoBucketLocal` para cadastrar a função que vai para _deploy_. Vamos copiá-la, colá-la logo abaixo de `functions` e adaptá-la, tirando do trecho "DoBucketLocal".

Além disso, em `handlers`, corrigiremos o caminho de "cadastro_batch/local/" para "cadastro_batch/aws/". Em `bucket`, trocaremos o nome de "alunos-csv-local" para "alunos-csv". O evento e as regras não mudam. O código ficará assim:

```yml
# ...

functions:
  cadastrarAlunos:
    handler: cadastro_batch/aws/index.cadastrarAlunos
    events:
      - s3:
          bucket: alunos-csv
          event: s3:ObjectCreated:*
          rules:
            - suffix: .csv

  simulandoUploadDoCsv:
    handler: cadastro_batch/local/index.simulandoUploadDoCsv
    events:
      - httpApi:
          path: /alunos/batch
          method: post
    enabled: false

  cadastrarAlunosDoBucketLocal:
    handler: cadastro_batch/local/index.cadastrarAlunosDoBucketLocal
    events:
      - s3:
          bucket: alunos-csv-local
          event: s3:ObjectCreated:*
          rules:
            - suffix: .csv

# ...
```

## Concedendo acesso

Nosso código está quase pronto. Há ainda um detalhe importante, quando lidamos com funções Lambda que interagem com outros recursos da AWS — no caso, um _bucket_ S3. Precisamos conceder permissão para essa função acessar o _bucket_ e conseguimos fazer essa configuração com Serverless Framework, neste arquivo `serverless.yml`.

Ao final de `provider`, vamos escrever `iam` para fazer referência ao usuário da AWS. Dentro de `iam`, escreveremos `role`, que conterá `statements`. Nele, teremos uma lista. Primeiramente, temos a propriedade `Effect: Allow` (em português, _allow_ significa permitir):

```yml
# ...

provider:
    name: aws
    runtime: nodejs18.x
    iam:
        role:
            statements:
                - Effect: Allow

# ...
```

Na linha seguinte, escreveremos `Action` para indicar a ação que será permitida, `s3:GetObject`, o comando de leitura de um arquivo. No caso, nossa função Lambda acessa o _bucket_ S3 para recuperar dados do arquivo CSV que fizemos upload:

```yml
# ...

provider:
    name: aws
    runtime: nodejs18.x
    iam:
        role:
            statements:
                - Effect: Allow
                  Action:
                        - s3:GetObject

# ...
```

Ainda dentro de `statements`, escreveremos `Resource`, que conterá outro item de lista. Essa citação é bem específica, `ard:aws:s3:::alunos-csv/*`:

```yml
# ...

provider:
    name: aws
    runtime: nodejs18.x
    iam:
        role:
            statements:
                - Effect: Allow
                  Action:
                        - s3:GetObject
                    Resource:
                        - arn:aws:s3:::alunos-csv/*

# ...
```

O asterisco ao final indica que a função Lambda poderá acessar qualquer arquivo do _bucket_ "alunos-csv". Vamos salvar as alterações e nosso código está pronto. No próximo vídeo, aprenderemos como fazer o _deploy_ da função.