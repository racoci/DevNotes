> [Arquivo `index.js` no GitHub](https://github.com/alura-cursos/serverless-framework-2-lambda/blob/aula-2/index.js)

Já estamos interagindo com o Bucket S3 local e criamos uma função chamada de `simulandoUploadDoCsv()` que responde a um evento HTTP. Essa função chama outra função chamada `fazUploadNoBucket()`.

Subindo o código do arquivo `index.js`, percebemos que a função `fazUploadNoBucket()` usa AWS SDK instalada no vídeo anterior e usamos um `S3Client` junto com o comando, e, assim que interagimos no Bucket.

Também vimos que a função lambda reage de acordo com o upload no Bucket local. Inclusive, vimos que o dado "12345" foi armazenado em uma pasta local. Com isso, a função lambda foi executada com sucesso.

> index.js

```javascript
// código omitido

module.exports.cadastrarAlunos = async (event) => {
    console.log("Função lambda executada a partir do evento do Bucket S3!")
};
```

No entanto, a função lambda atualmente só registra uma mensagem no console. Na verdade, nosso objetivo é cadastrar pessoas estudantes. Para isso, precisamos obter as informações de um arquivo CSV e, por isso, é necessário recuperar esses dados dentro da função.

Será que esses dados não estão disponíveis nos eventos que recebemos como parâmetro? Já voltamos para essa questão, antes disso, gostaria de esclarecer que o evento que estamos recebendo como parâmetro na função lambda **difere dos eventos que conhecemos das funções lambdas que reagem a requisições HTTP**.

Por exemplo, o `simulandoUploadDoCsv()` é uma função lambda que reage a um evento HTTP em que o evento pode ter parâmetro de busca. No entanto, o evento que vem por padrão possui um formato diferente.

Na função `cadastrarAlunos()`, vamos renomear o parâmetro `event` para `evento`. Em seguida, removeremos a frase dentro do `console.log` e substituiremos pelo parâmetro `evento`. O objeto evento possui algumas propriedades e faremos um `console.log` em uma delas, chamada `Records[0]`, sendo o `Record` uma lista de posição única, que acessaremos a posição zero.

O item da lista é um **objeto** que contém uma propriedade chamada S3. Nosso interesse está nas informações contidas dentro da propriedade S3.

> index.js

```javascript
// código omitido

module.exports.cadastrarAlunos = async (evento) => {
    console.log(evento.Record[0].s3)
};
```

Salvamos o arquivo, e no terminal integrado ao VS Code reiniciamos o servidor teclando "Ctrl + C" e digitamos o comando `sls offline`.

```undefined
sls offline
```

Ao teclarmos "Enter", obtemos como retorno:

> POST | [http://localhost:3000/alunos/batch](http://localhost:3000/alunos/batch)
> 
> POST | [http://localhost:3000/2015-03-31/functions/simulandoUploadDoCsv](http://localhost:3000/2015-03-31/functions/simulandoUploadDoCsv)

Realizamos a requisição para `alunos/batch` novamente a fim de verificar a execução da função lambda. Em seguida, fechamos o terminal integrado e abrimos o aplicativo **_Thunder Client_** no lado esquerdo do VS Code (atalho "Ctrl + Shift + R") para efetuar o `post` em `alunos/batch`. Dentro da requisição, clicamos no botão "Send" da rota [http://localhost:3000/alunos/batch](http://localhost:3000/alunos/batch).

Abrindo o terminal integrado novamente, temos o seguinte objeto com várias propriedades (`console.log` que acabamos de escrever):

```ruby
{
    s3SchemaVersion: '1.0',
    configurationId: 'testConfigId',
    bucket: {
        name: 'alunos-csv-local',
        ownerIdentity: { principalId: '4448416AC210E0'},
        arn: 'arn:aws:s3' : :alunos-csv-local'
    },
    {
    object: {
        key: 'teste.csv',
        sequencer: '18739211005',
        size: 5,
        eTag: '827ccb0eea8a706c4c34a16891f84e7b'
    }
}
```

Esse objeto possui uma propriedade chamada de `bucket` e outra chamada de `object`. A primeira possui um nome `alunos-csv-local` e a segunda possui uma chave chamada de `teste.csv`. Precisamos dessas duas informações para acessar os dados do arquivo.

Em relação à pergunta "Será que esses dados não estão disponíveis nos eventos que recebemos como parâmetro?", devemos ressaltar que o evento não contém diretamente as informações do arquivo que fizemos upload, como o 12345. Em vez disso, o evento fornece o nome do bucket e o nome do arquivo. A partir dessas informações, usaremos esses dados para acessar o bucket.

Vamos obter essas informações agora.

Ao retornar para o arquivo `index.js` na função `cadastrarAlunos`, em vez de utilizar o `console.log()` para exibir o valor contido em `evento.Record[0].s3`, vamos armazená-lo na constante chamada de `eventoS3`, por meio da expressão `eventoS3 = evento.Record[0].s3`. Na sequência escrevemos `const nomeBucket` que recebe `eventoS3.bucket.name`.

> index.js

```java
// código omitido

module.exports.cadastrarAlunos = async (evento) => {
  const eventoS3 = evento.Records[0].s3;

  const nomeBucket = eventoS3.bucket.name;
};
```

Na linha seguinte, inserimos a declaração `const chaveBucket = decodeURIComponent()`. Essa função é nativa do JavaScript e é utilizada para obtermos a string de forma que melhor atenda às nossas necessidades. Haverá uma atividade mais detalhada sobre essa função para você ler.

Dentro da função `decodeURIComponent()`, passamos a constante criada `evento.S3` e em seguida usamos a expressão `.object.key.replace()`. O método `replace()` é utilizado para substituir uma sequência de caracteres por outra em uma string. Passamos como parâmetro uma expressão regular, `/\+/g`, que a AWS sugere para que possamos obter o nome do arquivo do Bucket de forma mais eficiente. No `replace()` passamos um espaço vazio como segundo parâmetro, `""`.

Com isso, substituímos qualquer sinal de mais (+) por um espaço vazio do `object.key`.

> index.js

```javascript
// código omitido

module.exports.cadastrarAlunos = async (evento) => {
  const eventoS3 = evento.Records[0].s3;

  const nomeBucket = eventoS3.bucket.name;
  const chaveBucket = decodeURIComponent(eventoS3.object.key.replace(/\+/g, " "));

};
```

Em seguida, declaramos a constante `dadosArquivo`, que recebe o resultado da função `obtemDadosDoCsvDoBucket()`. Agora, será criada essa função `obtemDadosDoCsvDoBucket()` acima da `cadastrarAlunos()`.

Por se tratar de uma função assíncrona, usamos a sintaxe `async function obtemDadosDoCsvDoBucket()` para declará-la. As chaves `{}` delimitam o corpo da função. No corpo da função, vamos interagir com o bucket S3 local (servidor), para isso usaremos novamente a biblioteca `aws-sdk` para capturar de volta uma informação contida no bucket.

> index.js

```javascript
// código omitido

async function obtemDadosDoCsvDoBucket() {

}

module.exports.cadastrarAlunos = async (evento) => {
  const eventoS3 = evento.Records[0].s3;

  const nomeBucket = eventoS3.bucket.name;
  const chaveBucket = decodeURIComponent(eventoS3.object.key.replace(/\+/g, " "));

  const dadosArquivo = await obtemDadosDoCsvDoBucket()

};
```

Para o código não ficar muito grande, vamos refatorar uma parte da função `fazUploadNoBucket()`. A parte que vamos reutilizar é o `const cliente`, vamos selecionar da linha 4 até a 11 (`const cliente` até o `endpoint`) e teclar "Ctrl + X" para remover.

> index.js

```csharp
// código omitido

async function fazUploadNoBucket() {
  const cliente = new S3Client({
        forcePathStyle: true,
        credentials: {
            accessKeyId: "S3RVER",
            secretAccessKey: "S3RVER"
        },
        endpoint: "http://localhost:4569"
    });

// código omitido
```

Depois escrevemos de novo `const cliente`, só que agora ele irá receber o retorno da função `criaClienteS3Local()`. Também criaremos essa função agora, subindo o código escrevemos `function criaClienteS3Local() {}` e dentro do corpo dessa função teclamos "Ctrl + V" para colar o trecho copiado. Vamos remover o `const cliente` e inserir um `return` a `new S3Client`.

Assim, ficamos com:

> index.js

```csharp
// código omitido

function criaClienteS3Local() {
  return new S3Client({
    forcePathStyle: true,
    credentials: {
      accessKeyId: "S3RVER",
      secretAccessKey: "S3RVER"
    },
    endpoint: "http://localhost:4569"
  });
}

async function fazUploadNoBucket() {
  const cliente = criaClienteS3Local();

}

// código omitido
```

Retornamos à função assíncrona `obtemDadosDoCsvDoBucket()`, onde definimos no corpo da função a constante `cliente`, que corresponde ao código utilizado para estabelecer a conexão com o servidor local que está sendo reaproveitado: `const cliente = criaClienteS3Local()`.

Logo após definimos o comando que o cliente enviará para o S3, utilizando o código `const comando = new GetObjectCommand()`. Este `GetObjectCommand()` é um construtor específico da AWS que permite realizar a operação de leitura em um arquivo de um determinado bucket.

No parênteses do construtor, abrimos e fechamos chaves `{}` para abrir um objeto de configurações, passando o nome do bucket e a chave. Inclusive, podemos colocar como parâmetro o `nome` e a `chave` na função assíncrona `obtemDadosDoCsvDoBucket()`. Agora, na constante `dadosArquivo()` passamos `nomeBucket` e a `chaveBucket` (já declarados).

Voltando para `obtemDadosDoCsvDoBucket({})`, é necessário passarmos um objeto com duas propriedades: `Bucket`, que deve receber o `nome` recebido por parâmetro, e `Key`, que deve receber a `chave`.

Até o momento, temos:

> index.js

```javascript
// código omitido

async function obtemDadosDoCsvDoBucket(nome, chave) {
  const cliente = criaClienteS3Local();

  const comando = new GetObjectCommand({
    Bucket: nome,
    Key: chave
  });

}

module.exports.cadastrarAlunos = async (evento) => {
  const eventoS3 = evento.Records[0].s3;

  const nomeBucket = eventoS3.bucket.name;
  const chaveBucket = decodeURIComponent(eventoS3.object.key.replace(/\+/g, " "));

  const dadosArquivo = await obtemDadosDoCsvDoBucket(nomeBucket, chaveBucket)

};
```

Logo após o `comando`, precisamos incluir `const resposta = await cliente.send(comando);`. A `chave` que passamos como parâmetro em `obtemDadosDoCsvDoBucket(nome, chave)` é responsável por referenciar o nome do arquivo. Inclusive, por isso que passamos a `chaveBucket` na outra função também. Essa será a resposta do comando e faremos mais uma operação para obter os dados.

Escrevemos então `const dadosCsv = await resposta.Body.transformToString()`, sendo este último um método previsto pela `aws-sdk` para converter o corpo da resposta para string para usarmos no código. Por fim, retornamos com `return dadosCsv;`.

Voltando para `cadastrarAlunos`, colocamos no final do corpo da função um `console.log(dadosArquivo)`.

> index.js

```javascript
// código omitido


async function obtemDadosDoCsvDoBucket(nome, chave) {
  const cliente = criaClienteS3Local();

  const comando = new GetObjectCommand({
    Bucket: nome,
    Key: chave
  });

  const resposta = await cliente.send(comando);
  const dadosCsv = await resposta.Body.transformToString();

  return dadosCsv;
}

module.exports.cadastrarAlunos = async (evento) => {
  const eventoS3 = evento.Records[0].s3;

  const nomeBucket = eventoS3.bucket.name;
  const chaveBucket = decodeURIComponent(eventoS3.object.key.replace(/\+/g, " "));

  const dadosArquivo = await obtemDadosDoCsvDoBucket(nomeBucket, chaveBucket);

  console.log(dadosArquivo);
};
```

Abrindo o terminal integrado ao VS Code, reiniciamos o servidor com "Ctrl + C" e rodamos o comando `sls offline`.

Ao teclarmos "Enter", obtemos como retorno:

> POST | [http://localhost:3000/alunos/batch](http://localhost:3000/alunos/batch)
> 
> POST | [http://localhost:3000/2015-03-31/functions/simulandoUploadDoCsv](http://localhost:3000/2015-03-31/functions/simulandoUploadDoCsv)

Tudo certo, podemos fechar o terminal.

No _thunder client_, vamos fazer mais uma requisição para `post/alunos/batch` clicando no botão "Send". Retornou ao status `200 OK`, deu tudo certo.

Voltando ao terminal integrado, temos as informações dizendo que o upload do arquivo foi feito com sucesso (isso é feito a cada `send` no HTTP).

Parte do retorno indicado pelo instrutor:

> POST /alunos/batch (simulandoUploadDoCsv)
> 
> info: Stored object "teste.csv" in bucket "alunos-csv-local" successfully
> 
> info: PUT /alunos-csv-local/teste.csv?x-id=PutObject 200 26ms 0b … info: GET/alunos-csv-local/teste.csv?x-id=GettObject 200 5ms 5b
> 
> 12345

No caso, ao fazermos o upload de um arquivo CSV com o mesmo nome, ele é sobrescrito. Em seguida, ele nos retorna o método `PUT` indicando que fez o upload do arquivo para o bucket. Também retornou o método `GET` (realizado na função lambda) confirmado pelo plugin local do S3 e exibido no terminal como `12345`. Isso implica que conseguimos recuperar as informações do arquivo chamado de `teste.csv`.

Reformule o seguinte parágrafo de acordo com as normas da língua portuguesa, seja objetivo e claro, sem tirar os elementos principais do parágrafo:

Por fim, na função obtemDadosDoCsvDoBucket(), ao utilizarmos resposta.Body.transformToString(), o encoding padrão é utf-8. Se nenhum parâmetro de encoding é passado, o encoding padrão é utilizado. Isso é o que faz com que o valor 12345 seja obtido, mesmo quando o arquivo está como _buffer_. Vamos passar `utf-8` para deixar explícito.

> index.js

```javascript
// código omitido

async function obtemDadosDoCsvDoBucket(nome, chave) {
  const cliente = criaClienteS3Local();

  const comando = new GetObjectCommand({
    Bucket: nome,
    Key: chave
  });

  const resposta = await cliente.send(comando);
  const dadosCsv = await resposta.Body.transformToString("utf-8");

  return dadosCsv;
}

// código omitido
```

Após salvarmos, reiniciamos o servidor no terminal e executamos novamente o comando `sls offline` para verificar se o sistema está em execução. Faremos mais uma requisição para `/alunos/batch` clicando em "Send" e voltando ao terminal integrado, temos o mesmo retorno que antes com o nosso `12345`.

Até o momento, não foi exibido o conteúdo real do arquivo CSV. No próximo vídeo, começaremos a trabalhar com um arquivo CSV verdadeiro. No vídeo atual, foi demonstrado como recuperar informações do arquivo.

Te espero lá!