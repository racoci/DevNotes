**Antônio Evaldo:** Já aprendemos a simular o servidor S3 de forma local, utilizando alguns plugins para isso. Além disso, configuramos uma função que reage a uma requisição HTTP.

Ainda temos a pretensão de escrever um código nessa função que irá interagir com o servidor S3. Para isso, iremos juntar ambas as partes mencionadas acima.

## Fazendo upload no _Bucket_ local

Com o arquivo `index.js` aberto, vamos substituir o método `console.log()` da função `simulandoUploadDoCsv`. No lugar dele, escreveremos o operador `await` seguido de `fazUploadNoBucket()`.

```js
module.exports.simulandoUploadDoCsv = async (evento) => {
  try {
    await fazUploadNoBucket();

    return {
      statusCode: 200,
      body: JSON. stringify({
        mensagem: "Simulando upload de arquivo…"
      })
    };
  } catch (erro) {
    return {
      statusCode: erro.statusCode || 500,
      body: JSON. stringify(erro)
    };
  }
}

/* Código suprimido */
```

Criaremos uma função separada com esse mesmo nome, para organizar melhor as responsabilidades.

No início do arquivo, vamos escrever `async function`. Já sabemos ser uma função assíncrona, pois iremos utilizar operações assíncronas dentro dela.

Logo em seguida, colocamos o nome da função: `fazUploadNoBucket()`.

```js
async function fazUploadNoBucket() {

}

module.exports.simulandoUploadDoCsv = async (evento) => {
  try {
    await fazUploadNoBucket();

/* Código suprimido */
```

Dentro dessa função, iremos interagir de fato com o servidor S3 local. Quando queremos fazer essa interação com um Bucket S3, até mesmo um bucket de verdade, precisamos utilizar uma biblioteca da AWS chamada **AWS SDK**.

No bucket local, o processo será o mesmo. Utilizaremos a biblioteca AWS SDK para simular o máximo possível o ambiente de produção do bucket da AWS.

Para instalar a biblioteca, vamos abrir o terminal integrado. Primeiro encerraremos o servidor com o atalho "Ctrl + C".

Feito isso, vamos digitar o comando abaixo:

```css
npm install @aws-sdk/client-s3@3.295.0
```

Dessa forma, você estará em paridade com a versão instalada para utilizar ao longo do vídeo.

> O pacote AWS SDK será utilizado na **versão 3**, recomendada pela AWS. Há alguns tutoriais na internet de como instalá-lo, inclusive da própria AWS, porém eles estão desatualizados e são referentes à versão 2.
> 
> Existem algumas diferenças na instalação de uma versão para outra. Após esse vídeo, haverá um material explicando melhor sobre as diferenças.

Com o pacote instalado, vamos fechar o terminal integrado e retomar o trabalho na função `fazUploadNoBucket()` do arquivo `index.js`.

Dentro dela, escreveremos o código que irá se comunicar com o bucket local. Começaremos com a declaração `const` seguida de `cliente`, representando um cliente do S3 da AWS.

Ela irá receber um `new S3Client`. Conforme escrevemos, o VS Code sugere a importação automática do `S3Client` do pacote "@aws-sdk/client-s3". Iremos aceitá-la teclando "Enter".

Feito isso, a importação será feita automaticamente no começo do código, iniciado com a declaração `const` seguida de `S3Client` entre chaves, recebendo a `require()` que contém o caminho do pacote instalado.

```js
const { S3Client } = require("@aws-sdk/client-s3");

async function fazUploadNoBucket() {
  const cliente = new S3Client
}

module.exports.simulandoUploadDoCsv = async (evento) => {
  try {
    await fazUploadNoBucket();

/* Código suprimido */
```

> O `S3Client` será necessário para interagir com o bucket.

Após o `new S3Client`, vamos abrir parênteses, e dentro dele abrir chaves para passar um objeto de configurações que precisaremos escrever para conectar com o S3 local.

A primeira propriedade desse objeto será `forcePathStyle`, com o valor `true`. Essa propriedade é necessária para conectar com o bucket local.

```js
const { S3Client } = require("@aws-sdk/client-s3");

async function fazUploadNoBucket() {
  const cliente = new S3Client({
    forcePathStyle: true,
  })
}

/* Código suprimido */
```

Como segunda propriedade temos `credentials`, cujo valor é um objeto contendo uma propriedade chamada `accessKeyId`. Dentro dela, escreveremos a string `"S3RVER"`.

Logo abaixo, vamos adicionar outra propriedade, chamada `secretAccessKey`, na qual vamos colocar novamente a string `"S3RVER"`.

```js
const { S3Client } = require("@aws-sdk/client-s3");

async function fazUploadNoBucket() {
  const cliente = new S3Client({
    forcePathStyle: true,
    credentials: {
      accessKeyId: "S3RVER",
      secretAccessKey: "S3RVER"
  })
}

/* Código suprimido */
```

> Essas credenciais estão previstas pelo plugin do S3 local instalado. Precisamos escrever exatamente da forma exibida acima para conectar o cliente que estamos criando ao servidor S3 local.

Após `credentials`, vamos trazer a propriedade `endpoint`, cujo valor será a string `"http://localhost:4569"`. Esse número corresponde à porta que o próprio terminal indicou como disponibilizada pelo servidor S3 local.

Dessa forma, informamos ao `endpoint` que queremos conectar o cliente ao servidor local nessa porta específica.

```js
const { S3Client } = require("@aws-sdk/client-s3");

async function fazUploadNoBucket() {
  const cliente = new S3Client({
    forcePathStyle: true,
    credentials: {
      accessKeyId: "S3RVER",
      secretAccessKey: "S3RVER"
    },
    endpoint: "http://localhost:4569"
  });
}

/* Código suprimido */
```

Além do cliente, precisamos inserir um comando que o cliente deverá enviar para o servidor. Para começar, criaremos uma `const` chamada `comandoUpload`, pois queremos fazer o upload de um arquivo no servidor S3.

Esse comando irá receber um `new PutObjectCommand`, que novamente o VS Code sugere a importação automática da biblioteca "@aws-sdk/client-s3".

Vamos teclar "Enter" para aceitar a sugestão e importar automaticamente na primeira linha do arquivo.

```js
const { S3Client, PutObjectCommand } = require("@aws-sdk/client-s3");

async function fazUploadNoBucket() {
  const cliente = new S3Client({
    forcePathStyle: true,
    credentials: {
      accessKeyId: "S3RVER",
      secretAccessKey: "S3RVER"
    },
    endpoint: "http://localhost:4569"
  });

  const comandoUpload = new PutObjectCommand
}

/* Código suprimido */
```

Em seguida, vamos abrir parênteses após o `PutObjectCommand` para passar novamente um objeto de configurações.

A primeira propriedade do objeto se chama `Bucket`, com "B" maiúsculo. Nela nós indicamos o nome do bucket, que é `"alunos-csv-local"`.

> No arquivo `index.js`, é importante escrever o nome exatamente como indicado acima, pois esse foi o bucket que configuramos no arquivo `serverless.yml`.
> 
> **Relembrando:** neste arquivo, foi configurada uma função chamada `cadastrarAlunos` que irá reagir a um evento do `s3`, mais especificamente a um `bucket` chamado `alunos-csv-local`.
> 
> Justamente por isso, ao rodar o servidor S3 local, foi criada uma pasta com esse mesmo nome no diretório "buckets".

```js
/* Código suprimido */

  const comandoUpload = new PutObjectCommand({
    Bucket: "alunos-csv-local"
  })
}

/* Código suprimido */
```

A próxima propriedade se chama `Key`, também com inicial maiúscula, e seu valor pode ser qualquer um nesse momento. Vamos escrever, por exemplo, `"teste.csv"`. Este será o nome do nosso arquivo.

```js
/* Código suprimido */

  const comandoUpload = new PutObjectCommand({
    Bucket: "alunos-csv-local",
    Key: "teste.csv"
  })
}

/* Código suprimido */
```

Por fim, teremos a propriedade `Body`, novamente iniciada com letra maiúscula, e seu valor será `Buffer.from()`. Entre parênteses, também podemos colocar qualquer valor, como `"12345"`, por exemplo.

Isso porque vamos testar o upload de um arquivo nomeado `teste.csv`, cujo conteúdo será "12345".

```js
/* Código suprimido */

  const comandoUpload = new PutObjectCommand({
    Bucket: "alunos-csv-local",
    Key: "teste.csv"
    Body: Buffer.from("12345")
  })
}

/* Código suprimido */
```

> Posteriormente, vamos melhorar nosso código para utilizar de fato um arquivo `.csv` local do nosso computador. Assim, teremos uma melhor experiência de testes.
> 
> Por enquanto, começaremos de forma simples para avaliar se será feito o upload do arquivo `teste.csv` no Bucket S3 local.

Já temos os dois elementos necessários para interagir com o bucket: o `cliente` e o `comandoUpload`. Agora vamos juntar ambos.

Para isso, vamos utilizar o operador `await` seguido do método `cliente.send()`. Entre parênteses escreveremos `comandoUpload`.

```js
/* Código suprimido */

  const comandoUpload = new PutObjectCommand({
    Bucket: "alunos-csv-local",
    Key: "teste.csv"
    Body: Buffer.from("12345")
  });

  await cliente.send(comandoUpload)
}

/* Código suprimido */
```

O `send` significa "enviar" em português, então é como se o cliente estivesse enviando esse comando para o servidor S3.

> Esse código que estamos escrevendo é praticamente uma receita de bolo que a AWS disponibiliza para nos conectarmos com o Bucket S3. Então não se preocupe em decorar o código completo. Porém, é importante que você entenda o que está acontecendo em cada linha de código.

Antes de testar o funcionamento do código, vamos até a função `cadastrarAlunos` na linha 40 e apagar o `return`.

No lugar dele, vamos escrever o método `console.log()`, passando a mensagem `"Função lambda executada a partir do evento do Bucket S3!"`.

```js
/* Código suprimido */

module.exports.cadastrarAlunos = async (event) => {
  console.log("Função lambda executada a partir do evento do Bucket S3!")
};
```

Se tudo o que acabamos de escrever na função `fazUploadNoBucket()` funcionar corretamente, significa que o servidor S3 local fará o upload local do arquivo `teste.csv`, e irá emitir um evento que será escutado pela função `cadastrarAlunos` configurada no arquivo `serverless.yml`.

Agora vamos testar o código. Com o arquivo salvo, vamos abrir o terminal integrado e executar novamente o servidor local com o comando abaixo:

```undefined
sls offline
```

O terminal irá retornar praticamente o mesmo resultado anterior, sem nenhuma diferença significativa. Porém, vamos ver o que acontece ao fazer de novo a requisição HTTP para a rota "/alunos/batch".

Com a extensão _Thunder Client_ aberta, vamos clicar sobre a requisição POST feita anteriormente, salva no histórico.

Essa requisição está sendo realizada para o endereço "localhost:3000/alunos/batch". Podemos renomeá-la para "**POST /alunos/batch**" para melhorar a legibilidade.

Feito isso, vamos clicar no botão "Send".

```json
{
  "mensagem": "Simulando upload de arquivo…"
}
```

Continua aparecendo a mesma mensagem, então aparentemente não houve nenhum erro. Agora vamos abrir o terminal integrado para analisar o retorno.

```bash
POST /alunos/batch (λ: simulandoUploadDoCsv)
info: Stored object "teste.csv" in bucket "alunos-csv-local" successfully
info: PUT /alunos-csv-local/teste.csv?x-id=PutObject 200 34ms 0b
(λ: simulandoUploadDoCsv) RequestID: # ID omitido Billed Duration: 477 ms
Função lambda executada a partir do evento do Bucket S3!
(λ: cadastrarAlunos) RequestID: # ID omitido Billed Duration: 359 ms
```

Tivemos uma resposta dizendo que foi armazenado com sucesso o objeto `teste.csv` no bucket "alunos-csv-local".

Logo em seguida, temos a informação de que houve um método **PUT** em "/alunos-csv-local/teste.csv".

Além disso, conseguimos identificar duas execuções de funções lambda. Primeiro temos a função `simulandoUploadDoCsv`, executada a partir da nossa requisição HTTP. Depois temos a função `cadastrarAlunos`, também executada.

Na função `simulandoUploadDoCsv`, temos a frase "Função lambda executada a partir do evento do Bucket S3!", conforme definido anteriormente no método `console.log()`.

Isso significa que tudo funcionou corretamente. Conseguimos realmente interagir com o servidor S3 local, fizemos o upload do arquivo, e nossa função executou o `console.log()`.

No explorador de arquivos, conseguimos identificar 3 novos arquivos na pasta "alunos-csv-local", localizada no diretório "buckets". São eles:

- `teste.csv._S3rver_metadata.json`
- `teste.csv._S3rver_object`
- `teste.csv._S3rver_object.md5`

O arquivo `teste.csv._S3rver_metadata.json` informa que o tipo de conteúdo é `"application/octet-stream"`.

```json
{
  "content-type": "application/octet-stream"
}
```

Já o arquivo `teste.csv._S3rver_object` traz somente o conteúdo "12345", enquanto em `teste.csv._S3rver_object.md5` temos o conteúdo criptografado ("827ccb0eea8a706c4c34a16891f84e7b").

Note que a pasta de fato faz o papel de um Bucket S3 armazenando os arquivos. O mais interessante é que, assim que o arquivo foi armazenado, nossa função reagiu de acordo, e era exatamente isso que queríamos.

## Conclusão

Nossa integração está funcionando, mas cadastramos o conteúdo "12345". Na verdade, queremos utilizar arquivos `.csv`. Veremos como fazer isso a partir do próximo vídeo. **Te espero lá!**