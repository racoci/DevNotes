[00:00] Agora que já fizemos nossos primeiros testes e experimentos com o Jest, vamos fazer o seguinte: no editor de texto existem algumas outras formas com as quais podemos associar nossos testes e deixá-los um pouco mais descritivos, fazer um agrupamento deles que faça sentido na nossa aplicação.

[00:22] Uma das formas de agrupar esses testes, que é interessante utilizarmos, é o chamado _describe_. Podemos criar essa função chamada _describe_ e descrever alguns dos testes que nós vamos fazer.

[00:41] Eu vou abrir a chamada do _describe_ no topo do nosso arquivo, e podemos dar um título para um conjunto de testes. Eu vou dizer `describe(‘Testes dos cálculos de folha’)`.

[01:03] E dentro disso criamos uma _arrow function_ também. Depois desse título, abrimos uma _arrow function_ e um bloco. O conteúdo dessa _arrow function_ será os testes que desenvolvemos. Então, vou selecionar todos os testes e colá-los dentro desse bloco.

[01:30] Outra coisa é que a função “test” é um teste mesmo, só que tem uma forma mais sintática de utilizarmos. Em vez de “test”, podemos chamá-la de “it”. Esse nome novo fica parecido com o que fizemos na nossa versão, onde ao ler o título e a chamada parecia já ter um sentido. No inglês, seria algo parecido com “isso deve retornar a soma das horas extras”. Então é mais um passo para que os nossos testes virem uma documentação de como vamos trabalhar o nosso código e o nosso sistema.

[02:11] Eu vou trocar os nomes “test” por “it”, porque no inglês faz mais sentido. Conforme formos nos adequando, veremos que essa é uma forma mais sintática.

[02:27] E também podemos retirar e arrumar o que o Linter está pedindo. Damos “Ctrl + .” numa seleção onde tem um sublinhado vermelho para arrumar automaticamente os problemas.

[02:40] Lembrando que nas palavras reservadas do Jest, as funções como _it_ e _expect_ o Linter não sabe de onde vêm. Tem uma regra ou outra que você pode buscar para mitigar esse problema, talvez evitar ligar a regra _no-undefined_, apenas para os arquivos de teste ou coisas do tipo.

[03:03] Agora descrevemos um pouco melhor esses testes, completamos a suíte de testes, e veremos como podemos criar os relatórios dessa suíte de testes. No arquivo `package.json` do nosso projeto, existem os scripts, onde tem os roteiros, os comandos que demos para criar os testes.

[03:29] E nós vamos criar os testes que podem tanto ficar assistindo o nosso código e rodar automaticamente os testes conforme formos fazendo _commits_ ou alterações, como também criar um script que gere um relatório, uma página para ver como está indo a cobertura de testes e o nosso projeto em geral.

[03:51] Eu vou copiar a linha 7, com esse script de teste, e colá-la logo em seguida, com uma vírgula. Só que essa nova incidência eu vou chamar de `“test:watch”`. Ou seja, ele vai assistir o nosso código e ficar procurando por alterações ou _commits_ que tenham sido feitos.

[04:21] Vamos alterar o comando também de “test:watch”, para no final ter a _flag_ `--watch`. O Jest verá isso e saberá que tem que ficar olhando esse projeto, atualizando e rodando os testes quando necessário.

[04:36] Agora podemos voltar no terminal e chamar esse novo script que fizemos com o comando `npm run test:watch`. Podemos notar que temos uma nova tela embaixo da suíte de testes que rodou com sucesso, que tem a “Watch Usage”. Temos alguns comandos, por exemplo, apertar “a” para rodar todos os testes novamente, apertar “f” para apenas rodar os testes que falharam, podemos filtrar ou até sair desse modo de acompanhar o projeto.

[05:20] Para mostrar, eu vou voltar no `index.js` e trocar uma das funções. Na “calculaDescontos”, eu vou tirar o valor de subtração e vou somar o valor. Eu não fiz nada no terminal e ele já atualizou dizendo que um dos testes não passou. Ele diz que “Testes dos cálculos de folha deve descontar o valor do salário”. E ele diz que o valor recebido foi diferente do valor mostrado.

[05:51] Então está funcionando o _watch_, e assim temos uma forma mais prática de não precisar ficar executando toda vez os testes.

[06:00] Eu vou restaurar o nosso projeto, colocar a subtração novamente para subtrair os descontos. E no terminal os testes passaram novamente.

[06:10] Agora eu vou apertar “w” para ver os comandos, e ele diz no final que podemos apertar “q” para sair do modo de assistir. Apertei “q” e limpei a tela.

[06:21] Além do comando _watch_, que é muito prático, temos outra forma de ver essas informações, que é utilizando um relatório, que mostra nossa cobertura.

[06:34] Eu vou copiar a linha do comando “test:watch” e colá-la logo em seguida, chamando essa nova versão de “test:coverage”. E no final do comando vou adicionar `--coverage`, o que nos trará a cobertura de testes da nossa aplicação.

[06:59] Vou voltar no terminal e executar `npm run test:coverage`. Esse relatório já é um pouco diferente. Ele nos traz o sucesso dos testes, só que ele traz também uma tabela, que dá várias informações sobre o projeto. À esquerda, o primeiro valor dessa tabela é sobre quais são os arquivos que foram testados. Nós temos o `index.js`, que é onde temos o código da folha e dos cálculos.

[07:36] Temos depois a quantidade de _statements_, ou afirmações, em percentual, que foram percorridas. Por exemplo, toda vez que fazemos uma atribuição, uma conta ou algo tipo, fazemos _statements_.

[07:51] Temos um percentual de _branch_, e olhando podemos pensar que tem alguma coisa a ver com Git, falar que passou em todas as versões. Mas não tem nada a ver com Git. Essas _branches_ são as bifurcações de condicionais ou laços de repetição. Por exemplo, se você tem um _if_ e você fez um teste que pega o que está dentro do _if_ e não pega o que está do _else_, do outro bloco, isso te trará um problema. Ele vai dizer que uma bifurcação daquele código não foi percorrida e vai refletir nesse percentual.

[08:25] Depois temos o percentual de funções que foram chamadas, que, como o nome diz, é quais funções foram chamadas dentro daquela base de códigos. E depois um percentual de linhas que foram percorridas.

[08:39] Lembrando que não necessariamente a função testada foi chamada diretamente. Nós veremos um pouco de cenários assim, onde essas funções que estão aqui são funções que nós chamamos. Essas linhas podem ser linhas que apenas foram percorridas para outro lugar.

[08:57] Conhecemos um pouco a tabela, mas ela não é a única forma que temos de observar essas informações. Eu vou apertar o “q” para sair desse modo de assistir.

[09:09] Dentro do nosso projeto, teremos uma pasta nova na raiz, chamada “coverage”, com alguns arquivos e vários formatos, como `.xml`, .`js`, `.info`. Só que tem uma pasta dentro de “coverage” chamada “lcov-report”. Ou seja, ele traz um relatório que você pode acessar em uma página web.

[09:34] Nós podemos procurar pela página `index.html`, onde eu vou utilizar uma extensão do VS Code chamada Live Server para abrir essa página. Mas você poderia também navegar até a pasta ou colocar o caminho no seu navegador.

[09:49] Eu vou subir o servidor, e ele mostra todos os arquivos e aquela mesma informação que tinha na tabela, porém, de uma forma mais fácil de navegar. Você pode salvar essa pasta “coverage” e compartilhar com o gestor ou com alguém que tenha uma certa responsabilidade dentro daquele projeto.

[10:09] Ele vai dizer quais são os arquivos e quais foram aquelas funções e dados sobre cada um deles. E você pode até clicar no arquivo e ver um pouco a cara dele em um _preview_ de como ele está sendo testado.

[10:26] Nessa aula aprendemos um pouco mais sobre os testes unitários, conhecemos o Jest, vimos como descrever e agrupar os testes e as funções utilizando o _describe_ e o _it_. Também conhecemos como funcionam os relatórios e vimos a cobertura dos testes e como nossa aplicação está respondendo.

[10:46] Nas próximas aulas vamos colocar um projeto um pouco maior e veremos como esses testes serão implementados e como podemos ter algumas estratégias para testes síncronos da nossa aplicação. Espero você lá.